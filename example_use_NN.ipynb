{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4dGsGtW-CtZV"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import scipy.io\n",
    "import sklearn.preprocessing\n",
    "import time\n",
    "import hashlib\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_jWTSIQRCtZf"
   },
   "source": [
    "### define required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HFBK2uRuCtZk"
   },
   "outputs": [],
   "source": [
    "## convert the sparse matrix into a log x+1 normalized sparse tensor.\n",
    "def make_log_maxn_tensor(sparseMatrix):\n",
    "  # get the log transform...\n",
    "  sparseMatrix.data = np.log1p(sparseMatrix.data)\n",
    "  # normalize to the max...\n",
    "  sklearn.preprocessing.normalize(sparseMatrix, norm=\"max\", axis=1, copy=False)\n",
    "\n",
    "  # convert the filtered matrix to COO format\n",
    "  sparseMatrix = sparseMatrix.tocoo()\n",
    "  indices = np.mat([sparseMatrix.row, sparseMatrix.col]).transpose()\n",
    "\n",
    "  # make a sparse tensor and re-order it...\n",
    "  sparseMatrix = tf.SparseTensor(indices,sparseMatrix.data,sparseMatrix.shape)\n",
    "  sparseMatrix = tf.sparse.reorder(sparseMatrix)\n",
    "  return sparseMatrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Opo0KsGxrqrt"
   },
   "source": [
    "Copy the data from github to the VM.  We need to install git-lfs, because the count matrix is too large to be placed in github, so we use the git lfs (large file system. )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "gq_a06git3rK",
    "outputId": "71f622d0-7e64-4ed0-fe2a-c47263dc3165"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dirty_neurons_test.mtx.gz not found...  downloading....\n",
      "dirty_neuron_encoder.npy not found...  downloading....\n",
      "dirty_neurons_test_barcodes.csv.gz not found...  downloading....\n",
      "dirty_neurons_test_labels.csv.gz not found...  downloading....\n",
      "dirty_neurons_genes.csv.gz not found...  downloading....\n",
      "model neurons_doublets.model not found...  downloading from github ....\n",
      "model neurons_doublets.model.tgz not found...  downloading....\n",
      "model neurons_doublets.model downloaded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "x neurons_doublets.model/\n",
      "x neurons_doublets.model/variables/\n",
      "x neurons_doublets.model/saved_model.pb\n",
      "x neurons_doublets.model/assets/\n",
      "x neurons_doublets.model/variables/variables.data-00000-of-00001\n",
      "x neurons_doublets.model/variables/variables.index\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "# download the count matrix, labels, barcodes, genes, and the label encoder used during training...\n",
    "FILES=( dirty_neurons_test.mtx.gz dirty_neuron_encoder.npy dirty_neurons_test_barcodes.csv.gz dirty_neurons_test_labels.csv.gz dirty_neurons_genes.csv.gz )\n",
    "for FILE in \"${FILES[@]}\"\n",
    "do\n",
    "    if [ ! -f $FILE ]; then\n",
    "      echo \"$FILE not found...  downloading....\"\n",
    "      wget -q https://raw.githubusercontent.com/ArielLevineLabNINDS/Seq-Seek-classifyData/master/$FILE\n",
    "      #curl https://raw.githubusercontent.com/ArielLevineLabNINDS/Seq-Seek-classifyData/master/dirty_neurons_test.mtx.gz -o dirty_neurons_test.mtx.gz\n",
    "    else\n",
    "      echo \"$FILE found...\"\n",
    "    fi\n",
    "done\n",
    "\n",
    "# download the model\n",
    "FILE=neurons_doublets.model.tgz\n",
    "DIR=neurons_doublets.model\n",
    "\n",
    "if [ ! -d $DIR ]; then\n",
    "  echo \"model $DIR not found...  downloading from github ....\"\n",
    "  if [ ! -f $FILE ]; then\n",
    "      echo \"model $FILE not found...  downloading....\"\n",
    "      # if you are using a mac, comment out the wget command and uncomment out the curl command below\n",
    "      wget -q https://raw.githubusercontent.com/ArielLevineLabNINDS/SeqSeek-Classify-NN/master/$FILE\n",
    "      #curl https://raw.githubusercontent.com/ArielLevineLabNINDS/Seq-Seek-classifyData/master/$FILE -o $FILE\n",
    "  else\n",
    "      echo \"$FILE found...\"\n",
    "  fi\n",
    "  ## untar the file...\n",
    "  tar -xvf $FILE\n",
    "  rm $FILE\n",
    "\n",
    "  if [ ! -d $DIR ]; then\n",
    "    echo \"download failed!\"\n",
    "  else\n",
    "    echo \"model $DIR downloaded successfully\"\n",
    "  fi\n",
    "else\n",
    "  echo \"model $DIR found\"\n",
    "fi\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CxQpbfTuCtZr"
   },
   "source": [
    "# step two: \n",
    "### data loading...\n",
    "\n",
    "We are going to re-create the test data from the analysis.  In the future, I will create a \n",
    "dataset that is only test data, then I will try to create a function that will convert your data\n",
    "into the appropriat format.  After all, it's more fun to play with your data.  Keep in mind that\n",
    "this data is mouse spinal cord neurons and doublets only.  \n",
    "\n",
    "The input data is a sparse matrix created in R from Seurat data.  In Seurat, the count matrices are genes (rows) by cells (columns).  \n",
    "Tensorflow requires cells (rows) by genes (columns).  I performed the matrix transpose in R, so you won't see it here.\n",
    "\n",
    "note: it takes around 10 sec to read the data on my laptop (macbook pro 2018)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "D5hZaN1ECtZt",
    "outputId": "1231b185-d10c-4f8c-c6df-f69b0fb32038"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to read data files: 0.037 s\n",
      "all cell_ids from the labels == cell_ids from matrix: True\n",
      "time to read matrix: 10.147 s\n",
      "converting raw counts to CSR\n",
      "time to convert matrix: 0.053 s\n"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "genes = pd.read_csv(\"dirty_neurons_genes.csv.gz\",index_col=0)\n",
    "df = pd.read_csv(\"dirty_neurons_test_barcodes.csv.gz\",index_col=0)\n",
    "lbl = pd.read_csv(\"dirty_neurons_test_labels.csv.gz\",index_col=0)\n",
    "t1 = time.time()\n",
    "print(f\"time to read data files: {t1-t0:.3f} s\")\n",
    "print(f\"all cell_ids from the labels == cell_ids from matrix: {all(df.cell_id == lbl.cell_id)}\")\n",
    "\n",
    "t0 = time.time()\n",
    "dirty_neurons = scipy.io.mmread(\"dirty_neurons_test.mtx.gz\")\n",
    "t1 = time.time()\n",
    "print(f\"time to read matrix: {t1-t0:.3f} s\")\n",
    "\n",
    "if scipy.sparse.isspmatrix_coo(dirty_neurons):\n",
    "  print(\"converting raw counts to CSR\")\n",
    "  t0 = time.time()\n",
    "  dirty_neurons = dirty_neurons.tocsr()\n",
    "  t1 = time.time()\n",
    "  print(f\"time to convert matrix: {t1-t0:.3f} s\")\n",
    "\n",
    "x_test = make_log_maxn_tensor(dirty_neurons)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oDZc7dVdCtZ0"
   },
   "source": [
    "### load the label encoder\n",
    "The labels must be in the same order as model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n_4DCbqRCtZ1"
   },
   "outputs": [],
   "source": [
    "label_encoder = sklearn.preprocessing.LabelEncoder()\n",
    "label_encoder.classes_ = np.load('dirty_neuron_encoder.npy')\n",
    "ohe_encoder = sklearn.preprocessing.LabelBinarizer()\n",
    "ohe_encoder.classes_ = np.load('dirty_neuron_encoder.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jzsnBb-ECtaE"
   },
   "source": [
    "Normally at prediction time, we would not have labels.  But since we have them, let's use them..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wRspEI7eCtaF"
   },
   "outputs": [],
   "source": [
    "y_test = lbl.final_cluster_assignment.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lUMpQq6GCtaK"
   },
   "source": [
    "we now have the testing data and are ready to run it through the model...\n",
    "\n",
    "# Step Three:\n",
    "load the Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "CTj2dCV-CtaL",
    "outputId": "86ed4682-3af9-45c5-aecb-a8cfe163dfde"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_6\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 256)               6507520   \n",
      "_________________________________________________________________\n",
      "cell_type (Dense)            (None, 70)                17990     \n",
      "=================================================================\n",
      "Total params: 6,525,510\n",
      "Trainable params: 6,525,510\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('neurons_doublets.model')\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "kaxQ9uKiCtaP",
    "outputId": "da6a2538-d25b-45d6-d0c5-88abe69805a8"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>predicted_class</th>\n",
       "      <th>called_class</th>\n",
       "      <th>predicted_type</th>\n",
       "      <th>called_type</th>\n",
       "      <th>probability</th>\n",
       "      <th>agree</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>Garbage</td>\n",
       "      <td>Garbage</td>\n",
       "      <td>0.947911</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>Garbage</td>\n",
       "      <td>Garbage</td>\n",
       "      <td>0.964306</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>Garbage</td>\n",
       "      <td>Garbage</td>\n",
       "      <td>0.097964</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>Garbage</td>\n",
       "      <td>Garbage</td>\n",
       "      <td>0.957435</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Excit-01</td>\n",
       "      <td>Excit-01</td>\n",
       "      <td>0.538859</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2805</th>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>Excit-35</td>\n",
       "      <td>Excit-35</td>\n",
       "      <td>0.972334</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2806</th>\n",
       "      <td>62</td>\n",
       "      <td>62</td>\n",
       "      <td>Inhib-23</td>\n",
       "      <td>Inhib-23</td>\n",
       "      <td>0.951047</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2807</th>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>Excit-22</td>\n",
       "      <td>Excit-22</td>\n",
       "      <td>0.453575</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2808</th>\n",
       "      <td>35</td>\n",
       "      <td>39</td>\n",
       "      <td>Excit-35</td>\n",
       "      <td>Garbage</td>\n",
       "      <td>0.799530</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2809</th>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "      <td>Inhib-09</td>\n",
       "      <td>Inhib-09</td>\n",
       "      <td>0.898915</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2810 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      predicted_class  called_class predicted_type called_type  probability  \\\n",
       "0                  39            39        Garbage     Garbage     0.947911   \n",
       "1                  39            39        Garbage     Garbage     0.964306   \n",
       "2                  39            39        Garbage     Garbage     0.097964   \n",
       "3                  39            39        Garbage     Garbage     0.957435   \n",
       "4                   1             1       Excit-01    Excit-01     0.538859   \n",
       "...               ...           ...            ...         ...          ...   \n",
       "2805               35            35       Excit-35    Excit-35     0.972334   \n",
       "2806               62            62       Inhib-23    Inhib-23     0.951047   \n",
       "2807               22            22       Excit-22    Excit-22     0.453575   \n",
       "2808               35            39       Excit-35     Garbage     0.799530   \n",
       "2809               48            48       Inhib-09    Inhib-09     0.898915   \n",
       "\n",
       "      agree  \n",
       "0      True  \n",
       "1      True  \n",
       "2      True  \n",
       "3      True  \n",
       "4      True  \n",
       "...     ...  \n",
       "2805   True  \n",
       "2806   True  \n",
       "2807   True  \n",
       "2808  False  \n",
       "2809   True  \n",
       "\n",
       "[2810 rows x 6 columns]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict(x_test)\n",
    "cell_class = np.argmax(pred,axis=1)\n",
    "cell_type = label_encoder.inverse_transform(cell_class)\n",
    "called_class = label_encoder.transform(y_test)\n",
    "\n",
    "df = pd.DataFrame({\"predicted_class\":cell_class,\"called_class\":called_class,\"predicted_type\":cell_type,\"called_type\":y_test,\"probability\":np.max(pred,axis=1)} )\n",
    "df['agree'] = df.predicted_class == df.called_class\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# So you Now want to run your data...\n",
    "\n",
    "## background work Step 1\n",
    "lets take a second to talk about data formats..  Your data should be a sparse matrix in the for where Rows are the cells and the columns are the genes.  If you are running on Seurat data,  your Seurat object has the matrix saved in seuratObject.data.  In my version of Seurat, (Seurat_3.1.5) here is how I get the matrix.  If you have a better way, use that way.  The sparseMatrix format, called Matrix Market format, does not save the gene names or barcodes, so you need to write that out too.  \n",
    "\n",
    "```\n",
    "library(Seurat)\n",
    "library(dplyr)\n",
    "library(readr)\n",
    "library(Matrix)\n",
    "\n",
    "sparseMatrix <- GetAssayData(seuratObj,slot = \"counts\",assay = \"RNA\") \n",
    "writeMM(  t(sparseMatrix) ,\"myData.mtx\")\n",
    "write_lines(rownames(seuratObj),\"genes.txt\")\n",
    "write_lines(colnames(seuratObj),\"barcodes.txt\")\n",
    "\n",
    "```\n",
    "\n",
    "## backgroud work Step 2\n",
    "Congratulations, you saved a sparse matrix with gene names and .  The next step is to read the data into this colab notebook.\n",
    "\n",
    "I assume you are running this in colab.  If you are running this on your laptop in jupyter notebook/labs. The process is the same, but it may look different.  \n",
    "\n",
    "You need to upload the matrix to the notebook virtual machine. \n",
    "The left panel of the colab window should say \"Files\" at the top.  If it says \"Table of Contents, select the folder icon and you should see the files in colab. (See the screen shot below.  The red arrow points to the folder icon.\n",
    "Notice the upload button in the colab notebook, I circled it in blue in the screenshot. Click the upload button and select the files: myData.mtx, genes.txt, and barcodes.txt.  You still need the label encoder used during training of the neural network, and the neural network. The next code cell has code get the network and label encoder.  If you ran the code above, you already have the encoder and the network.  Don't worry, I check if you've already downloaded the files.\n",
    "\n",
    "Obviously, if you gave the files different names select the appropriate files.\n",
    "\n",
    "![](https://raw.githubusercontent.com/ArielLevineLabNINDS/SeqSeek-Classify-NN/master/colabScreenShot1.png)\n",
    "\n",
    "## Load the data into the code\n",
    "now we are ready to get started...\n",
    "Make sure that you ran the code above.  We're working to make it so you don't have to run the above code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dirty_neuron_encoder.npy found...\n",
      "model neurons_doublets.model found\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "FILE=dirty_neuron_encoder.npy\n",
    "if [ ! -f $FILE ]; then\n",
    "      echo \"$FILE not found...  downloading....\"\n",
    "      wget -q https://raw.githubusercontent.com/ArielLevineLabNINDS/Seq-Seek-classifyData/master/$FILE\n",
    "      #curl https://raw.githubusercontent.com/ArielLevineLabNINDS/Seq-Seek-classifyData/master/$FILE -o $FILE\n",
    "    else\n",
    "      echo \"$FILE found...\"\n",
    "    fi\n",
    "\n",
    "## download the model (same code as above)\n",
    "FILE=neurons_doublets.model.tgz\n",
    "DIR=neurons_doublets.model\n",
    "\n",
    "if [ ! -d $DIR ]; then\n",
    "  echo \"model $DIR not found...  downloading from github ....\"\n",
    "  if [ ! -f $FILE ]; then\n",
    "      echo \"model $FILE not found...  downloading....\"\n",
    "      # if you are using a mac, comment out the wget command and uncomment out the curl command below\n",
    "      wget -q https://raw.githubusercontent.com/ArielLevineLabNINDS/SeqSeek-Classify-NN/master/$FILE\n",
    "      #curl https://raw.githubusercontent.com/ArielLevineLabNINDS/Seq-Seek-classifyData/master/$FILE -o $FILE\n",
    "  else\n",
    "      echo \"$FILE found...\"\n",
    "  fi\n",
    "  ## untar the file...\n",
    "  tar -xvf $FILE\n",
    "  rm $FILE\n",
    "\n",
    "  if [ ! -d $DIR ]; then\n",
    "    echo \"download failed!\"\n",
    "  else\n",
    "    echo \"model $DIR downloaded successfully\"\n",
    "  fi\n",
    "else\n",
    "  echo \"model $DIR found\"\n",
    "fi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'genes.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-fdfd5dfbe136>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mgenes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"genes.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"gene\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"barcodes.txt\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"cell_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mt1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"time to read data files: {t1-t0:.3f} s\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    684\u001b[0m     )\n\u001b[1;32m    685\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 686\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    450\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    451\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 452\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    453\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    934\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    935\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 936\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    938\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1166\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"c\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"c\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1168\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1169\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"python\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1996\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"usecols\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1997\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1998\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1999\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2000\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'genes.txt'"
     ]
    }
   ],
   "source": [
    "t0 = time.time()\n",
    "genes = pd.read_csv(\"genes.txt\",names=[\"gene\"])\n",
    "df = pd.read_csv(\"barcodes.txt\",names=[\"cell_id\"])\n",
    "t1 = time.time()\n",
    "print(f\"time to read data files: {t1-t0:.3f} s\")\n",
    "\n",
    "t0 = time.time()\n",
    "data = scipy.io.mmread(\"myData.mtx\")\n",
    "t1 = time.time()\n",
    "print(f\"time to read matrix: {t1-t0:.3f} s\")\n",
    "if scipy.sparse.isspmatrix_coo(data):\n",
    "  print(\"converting raw counts to CSR\")\n",
    "  t0 = time.time()\n",
    "  data = data.tocsr()\n",
    "  t1 = time.time()\n",
    "  print(f\"time to convert matrix: {t1-t0:.3f} s\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Copy of example_use_NN.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
